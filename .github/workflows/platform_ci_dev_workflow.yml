name: platform_ci_dev_workflow

on:
  workflow_call:
    inputs:
      subscription_id:
        type: string
        description: "Azure Subscription ID"
        required: true
      resource_group_name:
        type: string
        description: "Azure Resource Group Name"
        required: true
      workspace_name:
        type: string
        description: "Azure ML Workspace Name"
        required: true
      env_name:
        type: string
        description: "Execution Environment"
        required: true
        default: "dev"
      flow_type:
        type: string
        description: "The flow use-case to execute"
        required: true
        default: "flows/category_1"
      sub_flows_to_execute:
        type: string
        description: "The sub flows to execute"
        required: true
        default: "flows/standard/baseline"
      runtime:
        type: string
        description: "The runtime to use"
    secrets:
      azure_credentials:
        description: "service principal authentication to Azure"
        required: true

jobs:
  flow-experiment-and-evaluation:
    name: prompt flow experiment and evaluation job
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Actions
        uses: actions/checkout@v4

      - name: Azure login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.azure_credentials }}

      - name: Configure Azure ML Agent
        uses: ./.github/actions/configure_azureml_agent

      #=====================================
      # Registers experiment dataset in Azure ML as Data Asset
      # Reads appropriate field values from data_config.json based on environment and data purpose
      #=====================================      
      - name: Register experiment data asset
        uses: ./.github/actions/execute_script
        with:
          step_name: "Register experiment data asset"
          script_parameter: |
            python -m llmops.common.register_data_asset \
            --subscription_id ${{ inputs.subscription_id }} \
            --resource_group_name ${{ inputs.resource_group_name }} \
            --workspace_name ${{ inputs.workspace_name }} \
            --data_purpose "training_data" \
            --flow_to_execute ${{ inputs.flow_type }} \
            --env_name ${{ inputs.env_name }}
          working_directory: src

      #=====================================
      # Executes Standard flow for a scenario
      # Generates Reports for each RUN as well as consolidated one
      # Execute a RUN for each unique variant combination (keeping default variant id for other nodes)
      # Loads appropriate experiment data from Azure ML data asset
      # Reads appropriate field values from mapping_config.json based on environment and evaluation flow name
      # Prompt Flow connections should pre-exist 
      # used automatic (serverless) runtime by default
      # writes the RUN ID in run_id.txt file. Used in next step
      #=====================================
      - name: Execute prompt flow bulk run
        uses: ./.github/actions/execute_script
        with:
          step_name: "Execute prompt flow bulk run"
          script_parameter: |
            python -m llmops.common.prompt_pipeline \
            --subscription_id ${{ inputs.subscription_id }} \
            --resource_group_name ${{ inputs.resource_group_name }} \
            --workspace_name ${{ inputs.workspace_name }} \
            --build_id ${{ github.run_id }} \
            --flow_to_execute ${{ inputs.flow_type }} \
            --sub_flows_to_execute ${{ inputs.sub_flows_to_execute }} \
            --env_name ${{ inputs.env_name }} \
            --data_purpose "training_data" \
            --output_file run_id.txt \
            --override_runtime ${{ inputs.runtime }}
          working_directory: src

      #=====================================
      # Reads run_id.txt file. Assigns it to variable RUN_NAME
      # RUN_NAME Used in next step for evaluation of flows
      #=====================================   
      - name: Read PromptFlow Runs
        shell: bash
        run: |
          readarray arr <"run_id.txt"
          run_name=${arr[0]}
          echo $run_name
          echo "RUN_NAME=${run_name}"  >> "$GITHUB_ENV"
          echo $PWD
        working-directory: src

      #=====================================
      # Registers evaluation dataset in Azure ML as Data Asset
      # Reads appropriate field values from data_config.json based on environment and data purpose
      #=====================================
      - name: Register evaluation data asset
        uses: ./.github/actions/execute_script
        with:
          step_name: "Register evaluation data asset"
          script_parameter: |
            python -m llmops.common.register_data_asset \
            --subscription_id ${{ inputs.subscription_id }} \
            --resource_group_name ${{ inputs.resource_group_name }} \
            --workspace_name ${{ inputs.workspace_name }} \
            --data_purpose "test_data" \
            --flow_to_execute ${{ inputs.flow_type }} \
            --env_name ${{ inputs.env_name }}
          working_directory: src

      #=====================================
      # Executes all Evaluation flows available for a scenario
      # Generates Reports for each RUN as well as consolidated one
      # Uses each RUN ID as input to run evaluation against
      # Loads appropriate evaluation data from Azure ML data asset
      # Reads appropriate field values from mapping_config.json based on environment and evaluation flow name
      # Prompt Flow connections should pre-exist 
      # used automatic (serverless) runtime by default
      #=====================================
      - name: Execute bulk run evaluations
        uses: ./.github/actions/execute_script
        with:
          step_name: "Execute bulk run evaluations"
          script_parameter: |
            python -m llmops.common.prompt_eval \
            --subscription_id ${{ inputs.subscription_id }} \
            --resource_group_name ${{ inputs.resource_group_name }} \
            --workspace_name ${{ inputs.workspace_name }} \
            --build_id ${{ github.run_id }} \
            --flow_to_execute ${{ inputs.flow_type }} \
            --sub_flows_to_execute ${{ inputs.sub_flows_to_execute }} \
            --env_name ${{ inputs.env_name }} \
            --data_purpose "test_data" \
            --run_id "$RUN_NAME" \
            --override_runtime ${{ inputs.runtime }}
          working_directory: src

      #=====================================
      # Published generated reports in csv and html format
      # Available as pipeline artifacts
      #=====================================
      - name: Archive CSV
        uses: actions/upload-artifact@v3
        with:
          name: evaluation-reports
          path: ./src/reports
